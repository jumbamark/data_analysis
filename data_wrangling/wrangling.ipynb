{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- Data wrangling is the process of gathering your data, accessing its quality and structure and cleaning it before you do things like analysis, visualizations or build perspective models using machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Outline\n",
    "1. Lesson 1: The Walkthrough\n",
    "2. Lesson 2-4: Gathering Assesing and Cleaning Data (in detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why wrangle data?\n",
    "- Data is being produced in large amounts so data savviness will only become more and more important in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Examples\n",
    "- Data Wrangling is serious business. The consequences from a lack of data wrangling can have a major impact\n",
    "    * __Financial Analyst__ - If you are creating models to make million dollar trades, your data better be clean or you'll go broke\n",
    "    * __Drug Company Scientist__ - If your company is about to start human trials for a life saving new drug and you need to determine the right dosage for humans based on your lab and animal tests, your data needs to be clean or your drug might not work and you could seriously hurt people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough and dataset\n",
    "- We start with brief introuductions to gathering, then assessing and then cleaning our data which are our three core steps in the data wrangling process\n",
    "- The dataset to be wrangled is a [dataset of 19000 online job posts from 2004 to 2015](https://www.kaggle.com/datasets/udacity/armenian-online-job-postings) that were posted through an Armenian human resource portal\n",
    "- The dataset is dirty and messy enough that you'll have wrangling work to do but alo clean enough that it wont give you a headache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather(Intro)\n",
    "__What is data gathering?__ \n",
    "- Gathering is sometimes called acquiring or collecting data.\n",
    "- Data sources; **Files, Database, scrapped off a website, API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather(Download)\n",
    "- Downloading can be done manually by clicking the download button or sometimes right clicking on a link and clicking \"Save file as\"\n",
    "- Best practice is to download files programmatically for **scalibility** and **reproducibility**\n",
    "\n",
    "1. Scalability - The ability of a process to handle an increasing scope of work. Imsgine you had a thousand files to download on a thousand different web pages instead of just one. It'd take an eternity to point and click a thousand times. You can do the same with a few lines of code\n",
    "2. Reproducibility - The ability of a process to produce the same results from identical inputs. Someone other than yourself will want to run your analysis later, so make downloading the datasets as easy to that person as possible\n",
    "- Reproducibility is also one of the main principles of [scientific methods](https://en.wikipedia.org/wiki/Scientific_method#Documentation_and_replication). You wanna be able to prove to people that your analysis and visualizations are legitimate plus the dataset on the web page it lives may change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather(unzip file)\n",
    "- Using code to unzip files makes your wrangling work more reproducibe than using an external program or clicking and unzipping the file\n",
    "* import the zip file library\n",
    "- `zipfile.Zipfile` is the class for reading and writing zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all contents from a zipfile\n",
    "with zipfile.ZipFile('archive.zip', 'r') as data:\n",
    "    data.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dc482c6d0e28c8f5d20d44b11c27cb87684a7eac92217e9d475a001f9707366"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('data_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
